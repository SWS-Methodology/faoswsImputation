%\VignetteIndexEntry{Statistical Working Paper on Imputation Methodology for the FAOSTAT Production Domain}
%\VignetteEngine{knitr::knitr}
\documentclass[nojss]{jss}
\usepackage{url}
\usepackage[sc]{mathpazo}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{breakurl}
\usepackage{hyperref}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{mathtools}
\usepackage{draftwatermark}
\usepackage{float}
\usepackage{placeins}
\usepackage{mathrsfs}
\usepackage{multirow}
%% \usepackage{mathbbm}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator*{\argmax}{\arg\!\max}

<<load-libraries, cache=FALSE, echo=FALSE>>=
## Load libraries
library(faoswsFlag)
library(faoswsUtil)
library(FAOSTAT)
library(data.table)
library(lattice)
library(splines)
library(RColorBrewer)

## time = 1:1000
## myBs = bs(time, df = 20, degree = 10, intercept = TRUE)
## plot.new()
## plot.window(xlim = c(0, max(time)), ylim = c(0, 1))
## for(i in 1:ncol(myBs)){
##     lines(time, myBs[, i], col = rgb(i/ncol(myBs), 0, 0))
## }


## time = 1:1000
## myBs = bs(time, df = 10, degree = 1, intercept = TRUE)
## ## plot.new()
## ## plot.window(xlim = c(0, max(time)), ylim = c(0, 1))
## par(mfrow = c(ncol(myBs), 1), mar = c(0, 5, 0, 0))
## for(i in 1:ncol(myBs)){
##     plot(time, myBs[, i], col = rgb(i/ncol(myBs), 0, 0), axes = FALSE,
##          ylab = paste0("Spline ", i), type = "l", lwd = 2)
## }

## Function to compute the yield as a ratio of production and area harvested.
computeRatio = function(numerator, denominator){
    as.numeric(ifelse(numerator == 0 | denominator == 0, NA,
                      numerator/denominator))
}

## A temporary flag table for the old flag system
oldFlagTable = 
    data.frame(flagObservationStatus = 
               c(" ", "*", "T", "P", "E", "F", "C", "M"),
               flagObservationWeights = 
               seq(1, 0, length = 8), 
               stringsAsFactors = FALSE)

## temporary function to read the data
read.example = function(file){
    tmp = read.csv(file = file, stringsAsFactors = FALSE)
    tmp2 = data.table(merge(tmp, 
        FAOcountryProfile[, c("FAOST_CODE", "FAO_TABLE_NAME")], 
        by.x = "areaCode", by.y = "FAOST_CODE"))
    setnames(tmp2, old = "FAO_TABLE_NAME", new = "areaName")
    tmp2[, productionSymb := ifelse(productionSymb == "", " ", productionSymb)]
    tmp2[, areaHarvestedSymb := ifelse(areaHarvestedSymb == "", " ",
                                       areaHarvestedSymb)]
    tmp2[productionSymb == "/", productionSymb := " "]
    tmp2[is.na(productionSymb), productionSymb := "M"]
    tmp2[productionSymb %in% c("P", "X", "*"), productionSymb := "T"]
    tmp2[productionSymb %in% c("C", "F"), productionSymb := "E"]
    tmp2[is.na(areaHarvestedSymb), areaHarvestedSymb := "M"]
    tmp2[areaHarvestedSymb == "/", areaHarvestedSymb := " "]
    tmp2[areaHarvestedSymb %in% c("P", "X", "*"), areaHarvestedSymb := "T"]
    tmp2[areaHarvestedSymb %in% c("C", "F"), areaHarvestedSymb := "E"]

    tmp2[, methodFlag := ifelse(productionSymb == "M", "u", "")]
    removeImputation(data = tmp2, value = "productionValue", 
                     observationFlag = "productionSymb",
                     methodFlag = "methodFlag",
                     imputedFlag = c("E", "T", "F"),
                     missingObservationFlag = "M")
    tmp2[, methodFlag := ifelse(areaHarvestedSymb == "M", "u", "")]
    removeImputation(data = tmp2, value = "areaHarvestedValue", 
                     observationFlag = "areaHarvestedSymb",
                     methodFlag = "methodFlag",
                     imputedFlag = c("E", "T", "F"),
                     missingObservationFlag = "M")
    remove0M(data = tmp2, value = "productionValue", 
             flag = "productionSymb", naFlag = "M")
    remove0M(data = tmp2, value = "areaHarvestedValue", 
             flag = "areaHarvestedSymb", naFlag = "M")    
    tmp2[, yieldValue := 
         computeRatio(productionValue, areaHarvestedValue)]
    setkeyv(tmp2, c("areaCode", "year"))
    tmp2[, yieldSymb := 
         aggregateObservationFlag(productionSymb, 
                                  areaHarvestedSymb,
                                  flagTable = oldFlagTable)]
    removeNoInfo(tmp2, value = "yieldValue", observationFlag = "yieldSymb",
                 byKey = "areaCode")
    tmp2[areaHarvestedSymb == "/", areaHarvestedSymb :=" "]
    tmp2[is.na(areaHarvestedSymb), areaHarvestedSymb :="M"]
    tmp2[is.na(productionSymb), productionSymb :="M"]
    tmp2[productionSymb == "X", productionSymb == "*"]
    tmp2[areaHarvestedSymb == "X", areaHarvestedSymb == "*"]
    tmp2[, productionSymb2 := ""]
    tmp2[, areaHarvestedSymb2 := ""]
    tmp2[, yieldSymb2 := ""]
    tmp2[, methodFlag := NULL]
    
    tmp3 = tmp2[areaCode != 357 & year >= 1992, ]
    ## tmp3 = tmp2[areaCode != 357, ]
    tmp3
}

wheat.dt = read.example("wheatSUA.csv")
grape.dt = read.example("grapesSUA.csv")
beef.dt = read.example("beefSUA.csv")
okra.dt = read.example("okraSUA.csv")
@ 

\title{\bf Statistical Working Paper on Imputation Methodology for the
  FAOSTAT Production Domain}

\author{Joshua M. Browning and Michael C. J. Kao\\ Food and Agriculture Organization \\ of the United Nations}

\Plainauthor{Joshua M. Browning and Michael C. J. Kao}

\Plaintitle{Statistical Working Paper on Imputation Methodology for
  the FAOSTAT Production Domain}

\Shorttitle{Imputation Methodology}

\Abstract{ 

  This paper proposes a new imputation method for the FAOSTAT
  domains based on linear mixed model and ensemble learning.
  
  The proposal provides a resolution to many of the shortcomings of the
  current approach, and offers a flexible and robust framework to
  incorporate further information to improve performance.
    
  A detailed account of the methodologies is provided. The linear
  mixed model demonstrates an ability to capture cross-country and
  cross-commodity information; meanwhile, the ensemble learning approach
  provides flexiblity and robustness where traditional imputation methods
  of applying a single model may have failed.
    
}

\Keywords{Imputation, Linear Mixed Model, Ensemble Learning}
\Plainkeywords{Imputation, Linear Mixed Model, Ensemble Learning}

\Address{
  Joshua M. Browning and Michael C. J. Kao\\
  Economics and Social Statistics Division (ESS)\\
  Economic and Social Development Department (ES)\\
  Food and Agriculture Organization of the United Nations (FAO)\\
  Viale delle Terme di Caracalla 00153 Rome, Italy\\
  E-mail: \email{joshua.browning@fao.org, michael.kao@fao.org}\\
  URL: \url{https://svn.fao.org/projects/SWS/RModules/faoswsImputation/}
}

\begin{document}

\section*{Disclaimer}
This working paper should not be reported as representing the views of
the FAO. The views expressed in this working paper are those of the
author and do not necessarily represent those of the FAO or FAO
policy. Working papers describe research in progress by the author(s) and
are published to elicit comments and to further discussion.\\

It is in the view of the author that imputation should be implemented
as a last resort rather than as a replacement for data
collection. Imputation itself does not create information; it merely
create observations based on assumptions.\\

This paper is dynamically generated on \today{} and is subject to
changes and updates.

\section{Introduction}
Missing values are commonplace in the agricultural production domain,
stemming from non-response in surveys or a lack of capacity by the
reporting entity to provide measurement. Yet a consistent and
non-sparse production domain is of critical importance to Food Balance
Sheets (FBS), thus accurate and reliable imputation is essential and a
necessary requisite for continuing work. This paper addresses several
shortcomings of the current work and a new methodology is proposed in
order to resolve these issues and to increase the accuracy of
imputation.\\

The primary objective of imputation is to incorporate all
available and reliable information in order to provide best estimates of
food supply in FBS.\\

Presented in table \ref{tab:swsflag} is a description of the existing
flags in the current Statistical Working System (SWS). In this
exercise, estimated and previously imputed data are marked as
either \textbf{E} or \textbf{I} and are the target values to
be imputed.\\

\begin{table}[h!]
  \label{tab:swsflag}
  \caption{Description of the flags in the Statistical Working System}
  \begin{center}
    \begin{tabular}{|c||p{12cm}|}
      \hline
      Flags & Description\\
      \hline
      & Official data reported on FAO Questionnaires from countries\\
      I & Imputed figure\\
      T & Extrapolated/interpolated\\
      M & Not reported by country\\
      E & Expert sources from FAO (including other divisions)\\
      \hline
    \end{tabular}
  \end{center}  
\end{table}

\section{Exploratory Data Analysis}

We first take a visualization tour of the data to grasp an
understanding of the underlying pattern of the production domain and
the relationship between the variables. The series commence in 1992
and continues to 2011.

\subsection{Yield}

The next three graph depicts the yield of four selected commodity from
different commodity group, wheat, grape, okra and beef.\\

From the graphs, we can first observe that there is a general
increasing trend in the yield accross all countries and commodities
illustrated. Similar stories are observed in almost all commodities
that has been studied during the development of the methodology. This
is a result of continuous advancement in both technology and
agricultural practice driven by research \& development. Improved
irrigation provides crop with sufficient and uninterrupted water
source, while tailored compound feed provide the precise nutrient
requirements ensuring the livestocks consume the optimal diet for
growth. Regardless whether these practice are sustainable or
beneficial, there are strong evidence of increased productivity over
time.\\

Nevertheless, just like all available technology such as internet, the
distribution is far from perfect. The adoption of technology depends
on the access which may be hindered by the presence of patents, or
restriction imposed by service providers. Imperfect information and
limit financial resources are also major obstacles for embracing the
new developments, this is particularly true for countries where the
majority of the producers are smallholders or rural farming.\\

Furthermore, producers faces different constraints and cost. Countries
such as Brazil and Russia which has a large amount of arable land does
not bear the same cost for land acqusition in comparison to small
states such as the Netherlands. The cost translates to different
pressure to improve productivity and yield. Inovations required are
also different for countries, wheat breeding for the development of
drought and diease resistent variety were crucial to withold
Australia's dry climate.  \\

Despite the differences among the countries branching from various
combination of technological advancement and economic condition, these
factors all contribute towards a positive improvement in productitive
which can be estimated as an aggregated mixture effect.\\

Forces of nature also play a vital role in the determination of crop
productivity. However, unlike technological advancement, precipitation
and temperature are associated more closely to year-to-year variation.\\

<<wheat-yield-explore, fig.width=15, fig.height=12, fig.cap='This figure illustrates the yield of wheat accross all countries, it provide strong support to the facts stated. First of all, we can observe the concordant increasing trend accross all country where technological innovation such as improved seed, and synthetic nitrogen fertilizer contributed to the increase in productivity. Yet at the same time, we can also observe that the rate of growth differ between countries. The single yield spike in Zambia raises concern on data quality.', fig.pos='!ht', echo = FALSE>>=
xyplot(yieldValue ~ year|areaName, data = wheat.dt, type = c("g", "l"),
       xlab = "", ylab = "Wheat Yield (1000 Hg/Ha)")
@
\newpage

<<grape-yield-explore, fig.width=15, fig.height=12, fig.cap='Unlike the yield of wheat, the yield for grape has remain rather constant over time except a few selective country such as Peru and Azerbaijian. There are a few spikes observed, namely Iraq, the invasion of Iraq may have contributed to the negative shock. The considerable fall in the yield for India is of unknown cause, and potentially a data entry error.', fig.pos='!ht', echo = FALSE>>=
xyplot(yieldValue ~ year|areaName, data = grape.dt, type = c("g", "l"),
       xlab = "", ylab = "Grape Yield (1000 Hg/Ha)")
@ 
\newpage

<<okra-yield-explore, fig.width=15, fig.height=12, fig.cap='Shown in this graph are the yield of Okra over time. We can observe that the data is extremely sparse, further the quality of the data is questionable. Yield growth from less than 10 Hg/Ha to greater than 30 Hg/Ha in a single year for both Bahrain and Senegal is deemed suspicious.', fig.pos='!ht', echo = FALSE>>=
xyplot(yieldValue ~ year|areaName, data = okra.dt, type = c("g", "l"),
       xlab = "", ylab = "Okra Yield (1000 Hg/Ha)")
@ 
\newpage

<<beef-yield-explore, fig.width=15, fig.height=12, fig.cap='Similar to grape, the carcass weight per animal of beef also shows the same pattern of a very modest improvement in yield with a handful of exceptions.', fig.pos='!ht', echo = FALSE>>=
xyplot(yieldValue ~ year|areaName, data = beef.dt, type = c("g", "l"),
       xlab = "", ylab = "Beef Yield (1000 Hg/An)")

@
\newpage

\FloatBarrier
\subsection{Production and Area Harvested}

Although yield plays a vital role in the production process, the
actual quantity of production is usually dictated by the area sown and
harvested. The illustrations in this section shows that the production
series is usually dominated by how much area was planted and
harvested.\\

In contrast to the simple mechanism of yield where all dominant
factors contribute towards improving the productivity, the mechanism
of production is much more unpredictable. \\

Production is determine by area harvested and hence area sown in the
previous period by the farmer. Which ultimately depends on the
perception of information and subjective judgement of the
producer. Production can increase or decrease production as a response
to the state of the market, wheat field can be substitue to harvest
sorghum if prices are expected to be high. Further, individual
entities faces different risk profile, even under the assumption of
all producers are profit-seeking the risk profile may alter the
portfolio of products held by the producer. Markets has been known to
be difficult to forecast, let alone the prediction of human judgement
is just shy of impossible under curren state of understanding.\\

Only in cases where the commodity is a major staple or exporting item,
we can observe simple trend explained by the continuous increase in
demand. On the other hand, commodities which are of relative lesser
importance, the pattern of the production may display unpredictable
erratic behaviour.\\

<<wheat-production-area-explore, fig.width=15, fig.height=20, fig.cap='Wheat production and area harvested by country. The figure shows that excluding several producers such as India, Nepal, and Pakistan which has a stable trend in production, both the production and area of most countries display erratic behavior.', fig.pos='!ht', echo = FALSE>>=
xyplot(productionValue + areaHarvestedValue ~ year|areaName, 
       data = wheat.dt, type = c("g", "l"),
       xlab = "", ylab = "", 
       scales = list(y = list(relation = "free", draw = FALSE)),
       auto.key = 
       list(text = c("Production", "Area harvested")))
@
\newpage

<<grape-production-area-explore, fig.width=15, fig.height=20, fig.cap='In contrast to wheat, the area for grape is much more stable, a character of tree which takes year to plant and nuture and the alteration of the land use is much more difficult. Nonetheless, the production also display different trends over different time period', fig.pos='!ht', echo = FALSE>>=
xyplot(productionValue + areaHarvestedValue ~ year|areaName, 
       data = grape.dt, type = c("g", "l"),
       xlab = "", ylab = "", 
       scales = list(y = list(relation = "free", draw = FALSE)),
       auto.key = 
       list(text = c("Production", "Area harvested")))
@ 
\newpage

<<okra-production-area-explore, fig.width=15, fig.height=20, fig.cap='Even more so than both wheat and grape, the production of okra appears to demonstrate unpredictable trends and shocks.', fig.pos='!ht', echo = FALSE>>=
xyplot(productionValue + areaHarvestedValue ~ year|areaName, 
       data = okra.dt, type = c("g", "l"),
       xlab = "", ylab = "", 
       scales = list(y = list(relation = "free", draw = FALSE)),
       auto.key = 
       list(text = c("Production", "Area harvested")))
@ 
\newpage

<<beef-production-area-explore, fig.width=15, fig.height=19, fig.cap='Of all the production, livestock meat such as beef and veal may have been the easiest to predict and impute. There is continuing demand around the world for meat, while shift in production is usually difficult due to the high expenditure in machinery capital. With this being said, we can likewise observe shocks and or period of contraction or expansion over time.', fig.pos='!ht', echo = FALSE>>=
xyplot(productionValue + areaHarvestedValue ~ year|areaName, 
       data = beef.dt, type = c("g", "l"),
       xlab = "", ylab = "", 
       scales = list(y = list(relation = "free", draw = FALSE)),
       auto.key = 
       list(text = c("Production", "Area harvested")))
@ 
\newpage


%% One worthnoting point is that production which display simple pattern
%% are typically commodities in countries which are highly commercialized
%% or important for consumption as staple or traded for financial
%% resources. These item are rarely the one we need to impute as they are
%% generally complete, it is commodities of lesser importance that are
%% required to be imputed and as a result that are often harder due to
%% their unstable demand and nature.\\

\section{Proposed Methodology}

The imputation of missing observations is traditionally done via a model.  For example, we may consider a simple global mean model where we compute the mean of all available observations and use that value to impute missing observations.  Alternatively, we could use a more complex model (i.e. linear/exponential/logistic regression, spline, etc.), fit the model to the available data, and then estimate missing values using this model.  However, this approach has two problems.  First, we may choose a poor model and thus obtain poor estimates.  Second, we have to specify which model to use for each set of data, and this could be very tedious if we have many time-series to impute.  To avoid this problems, we consider ensemble models.

\subsection{Ensemble Imputation}

Ensemble learning refers to the process of building a collection of
simple base models or learners which are later combined to obtain a
composite model or prediction.  One of the most famous applications of ensemble learning was the prediction of movie ratings held by Netflix in which the
top two performers both used an ensemble of different models.  Ensembles are very popular in the data-mining community because of their ability to combine multiple models and come up with an estimate that is better than any of the individual models.\\

The method consist of two steps:
\begin{enumerate}
  \item Building multiple models/learners.
  \item Combining the models or predictions.
\end{enumerate}

The ensemble method reduces the risk of choosing a poor model as we are averaging multiple models.  Thus we reduce the risk of implementing a single model which may produce poor imputations for a certain subset of data. Moreover, model selection is unecessary, since all model are included in the final ensemble.\\

Thomas Dietterich describes several problems in machine learning in his paper ``Ensemble Methods in Machine Learning'' (see \url{http://www.eecs.wsu.edu/~holder/courses/CptS570/fall07/papers/Dietterich00.pdf}), and he also discusses how using an ensemble can reduce the errors from the following three issues.

\begin{itemize}
  \setlength{\itemindent}{1in}
  \item[\textbf{Statistical:}] A lack of data may allow multiple models to fit the training set well.
  \item[\textbf{Computational:}] Optimization procedures occasionally converge to local solutions instead of the global solution.
  \item[\textbf{Representational:}] It may not be possible to model the true phenomenon with a known model.
\end{itemize}

\begin{figure}[!ht]
  \centering
  \includegraphics[scale = 0.7]{dietterich.png}
\end{figure}

The statistical problem refers to the lack of data to support a
particular hypothesis. The problem can be formulated as finding the
best hypothesis among competing models in the space
$\mathbf{\mathcal{H}}$. In the top left graph of the
depiction from Dietterich we see a blue boundary, and the idea is that all
models within this boundary will give the same fit to the training data.
Thus, there is insufficient information to determine which one is better.
By combining the models, we reduce the risk of choosing a terrible model.
For example, if we only observe two data points for a country, then
fitting a linear line or a log curve can both give the same accuracy
on the training data and we may have no information to distinguish
between the two.\\

The second problem is that some models are fit by optimizing some cost function.  These numerical algorithms can often converge to local solutions instead of the global solution.  The top right graph from Dietterich represents this problem, with points h1, h2, and h3 representing the local solutions and f the true global solution.  Thus, combining the multiple fits should get us closer to the true optimum f.  At the time of writing this vignette, no models which use this numerical optimization are present in the default methodology; however, we could introduce such models in the future (for example, a neural network).

The final problem, representational, refers to the fact that the true
function $f$ can not be represented by any of the individual models. However, by
combining the models we may expand the space of representable
functions and more closely approximate the true function $f$.  For example, if
the production of a country has been growing at a linear rate in the
distant past but has expanded rapidly recently, then neither a linear or
exponential model will provide a satisfactory result. However, an
ensemble combining a linear and exponential model will provide a better
solution by capturing different characteristics of the data.\\

From an implementation point of view, the algorithm is adaptive and will not need constant updating.  For example, If the data generating mechanism changes in the future, the next fit of the ensemble will shift weights to models which better represent the data and thus it will not be necessary to constantly monitor and update the methodologies/models manually.

\subsection{Description of Models}

This section describes the different base learners for the ensemble methodology, and they are listed in increasing order of complexity. An effective ensemble will have base models as diverse as possible. If there is no diversity and all models generate similar results, then little is gained by combining these models and the ensemble model will not be much of an improvement from an individual model.\\

\begin{itemize}
  \item [Mean:] Mean of all observations
  \item [Median:] Median of all observations
  \item [Linear:] Linear Regression
  \item [Exponential:] Exponential function
  \item [Logistic:] Logistic function        
  \item [Naive:] Linear interpolation followed by last observation
    carried forward and first observation carried backward.
  \item [ARIMA:] Autoregressive Integrated Moving Average model
    selected based on the AICC, and imputation via Kalman Filter.
  \item [LOESS:] Local regression with linear models and model window
    varying based on sample size.
  \item [Splines:] Cubic spline interpolation.
  \item [MARS:] Multivariate Adaptive Regression Spline
  \item [Mixed Model:] Linear mixed model with time as a fixed effect
    and country as the random effect.
\end{itemize}

\subsection{Model ``Level''}

We wish to be able to construct models of varying levels of complexity, and in doing so we'd like to be able to build very localized models as well as more general/global models.  One way of doing this is by restricting the training dataset for each model constructed.  For example, if we have fairly good data availability for a particular country/commodity, then we may wish to use only that country's data when building a model.  However, if one country/commodity only has a few valid observations, then we may need to use global trends for that commodity to model that country/commodity more accurately.

In the current code, there are two ``levels'' for constructing a model: ``countryCommodity'' and ``commodity.''  The first, ``countryCommodity,'' means that the model will only use data for that specific country/commodity pair in the fitting of the model.  Thus, we will construct a different model for each different country/commodity time series.  Most of the implemented models fall under this methodology: mean, linear, exponential, logistic, naive, loess, splines, and MARS.  

The ``commodity'' level means that the model uses data for that commodity but considers all countries.  The mixed model follows this approach in that all data for one commodity is used to fit a mixed effects model (where year is considered a fixed effect and country a random effect).

\subsection{Extrapolation}

The ensembling process makes use of many different models, but we must be careful in considering what kinds of models to use in which scenarios.  As a simple example, suppose a country has production values of 1, 2, and 4 in three consecutive years and then twenty years of missing data.  If we fit an exponential model to this data, we'll be estimating production values over 4 million at the end of the twenty year period!  In addition, other models (such as splines and LOESS) don't extrapolate well.

Thus, for each model, we have an extrapolation parameter.  This value allows the user to control how far outside the range of the data a particular model can be used.  Using this functionality allows us to prevent extrapolating with models that clearly shouldn't be used outside the range of the data while still making use of these models for interpolation purposes.

\subsection{Computation of Weights}

To construct an ensemble, we use a weighted average of all of the input models.  However, we must determine a meaningful way for computing weights, as models which perform poorly shouldn't receive as much weight as models which fit the data well.  A simple approach is to compare, for each valid observation, the model estimate and the true value.  If we average the error between these two values across all valid observations, we get an estimate for the error of the model.  Then, we can use this error to compute the model weights:

$$w_i = 1/e_i / \sum_{j = 1}^n 1/e_j$$

where $w_i$ is the weight of the $i$th model, $e_i$ is the error of the $i$th model, and $n$ is the total number of models.   Thus, models with smaller errors receive more weight in the final ensemble, and the summation on the bottom of the above formula ensures that the weights sum up to one (ensuring that our weighted sum is in fact a weighted mean).  This approach is possible in the current code by setting the errorType to ``raw'' in the imputation parameters list.

However, the above approach is not ideal.  The problem lies in the fact that complex models generally will fit the training data better because they are more complex.  In reality, we want to know if these models are more accurate at predicting unknown values, and thus we need a way to measure how effectively these models can predict on new observations.  To accomplish this, we use cross-validation.

With cross-validation, the observed data are split into $k$ different groups (often $k=10$, and this is the default for this package as well).  Then, for each group $i$, we build a model using all of the observed data except for those in group $i$ and we measure how well this model estimates the data in group $i$.  If we average this error across all $k$ groups, we get a measure for how well this model predicts on our particular dataset.  We perform this cross-validation error estimation for each of our different models, and then we compute model weights via

$$w_i = 1/e_i / \sum_{j = 1}^n 1/e_j$$

The formula here is the same as the one above, but now the errors are the average cross-validation errors instead of the errors on the training set.

% \section{Case Studies}
% 
% Show a bunch of examples where certain products were imputed.
% 
% \section{Simulation Study}
% 
% If this is a priority, do a simulation study to show that the method of
% imputation via ensembles is effective.

\end{document}